---
layout: main_layout
title: Latent Semantic Analysis
---

<div class="main">
    <h2>{{page.title}}</h2>

    <p class="indent">
        Within the realm of natural language processing,
        <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a> (LSA) is a tool that is used to analyze the relationships between a set of documents
        and the words that they contain. This post will take an in-depth look at LSA and show how it can be used to sort
        large collections of documents.
    </p>

    <blockquote>

        <p class="indent">
            Repository for the Python code that was used in this post:
            <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Example">LSA Walkthrough</a>
            <br/>
        </p>

    </blockquote>


    <p class="indent">
        Consider the four following documents:
    </p>

    <img src="\pictures\Documents.svg" title="Example Documents" class="center_image" />

    <p class="indent">
        After reading through these documents, it’s easy to see that they could be sorted into two different categories. For the
        sake of simplicity, let’s call these categories
        <i>pets</i> and
        <i>travel</i>.
    </p>

    <p class="indent">
        This is a rather trivial conclusion for us to draw since there is a clear difference in subject between the first two documents
        and the last two documents. However, for a computer, identifying the differences is not a simple task. What techniques
        do we have at our disposal to make a computer to recognize these kinds of differences?
    </p>

    <p class="indent">
        One common techinque starts by looking at the frequencies at which words appear inside of each document. Intuitively, by
        grouping together the documents that use the same words frequently, we can naively sort the documents. For example,
        the first two documents use the words
        <i>cats</i> and
        <i>dogs</i> frequently while the other two documents do not, hence it may be reasonable to assume that these two documents
        are similar to each other and can be placed into the same category.
    </p>


    <p class="indent">
        To quantify this idea, it is common practice to convert a corpus into what is known as a
        <i>document-term</i> frequency matrix. Each row of this matrix represents a document and the columns represent the words
        that appear inside of these documents. By letting each entry of this matrix represent the frequency at which these
        words appear inside of each document, we have a consise way of representing these documents based off of their word
        frequencies. The rows of this matrix are often referred to as
        <i>document vectors</i>.
    </p>

    <p class="indent">
        Here is the document-term frequency matrix for our example documents (note that I filtered out words like
        <i>and</i> and
        <i>the</i> before I created the matrix by using a
        <a href="https://en.wikipedia.org/wiki/Stop_words">stop list</a>):
    </p>

    <iframe width=100% height="350" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/2.embed"></iframe>

    <p class="indent">
        From this representation, we can clearly see which documents used the same words frequently, but it's not abundantly clear
        which documents belong to the same category.
    </p>

    <iframe width=100% height="350" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/4.embed"></iframe>

    <iframe width=100% height="400" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/6.embed" class="center_heatmap"></iframe>

</div>