---
layout: main_layout
title: Latent Semantic Analysis
---

<div class="main">
    <h1>{{page.title}}</h1>

    <p class="indent">
        Within the realm of natural language processing,
        <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a> (LSA) is a tool that is used to analyze the relationships between a set of documents
        based on the words that they contain. This post will take an in-depth look at LSA and show how it can be used to sort
        large collections of documents.
    </p>

    <p class="indent">
        The Python code that was used in this post can be found here:
        <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Example">LSA Walkthrough</a>
        <br/>
    </p>

    <p class="indent">
        To understand Latent Semantic Analysis, lets walk through an example of how it can be used to sort the following four (very unrealistic) documents:
    </p>

    <div class="center">
        <img src="\pictures\Documents.svg" title="Example Documents"/>
    </div>
    
    <p class="indent">
        After reading through these documents, it’s not hard to see that they could be sorted into two different categories. For the
        sake of simplicity, let’s call these categories
        <i>pets</i> and
        <i>travel</i>.
    </p>

    <p class="indent">
        This is a rather trivial conclusion for us to draw since the subject of the first two documents is clearly different than the last two, but for a computer,
        identifying these subject differences is no simple task. What techniques do we have at our disposal to make a computer to recognize these kinds of differences?
    </p>

    <h2>Document-Term Frequency and Cosine Similarity</h2>

    <p class="indent">
        A common starting point begins by looking at the frequencies at which the words inside of a document appear inside of the others. Intuitively, by grouping together
        the documents that use the same words frequently, we can (naively) sort the documents. To do this, it is common practice to convert a collection of documents into what 
        is known as a <i>document-term</i> frequency matrix (in my code and from here on out, I will refer to it as a "frequency matrix").
    </p>


    <p class="indent">
        Each row of this matrix represents a document and the columns represent the words
        that appear inside of these documents. By letting each entry of this matrix represent the <i>frequency</i> at which these
        words appear inside of each document, we have a consise way of representing these documents based off of their word
        frequencies.
    </p>

    <p class="indent">
        Here is the document-term frequency matrix represented as a heat map for our example documents:
    </p>

    <div class="heatmap">

        <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/2.embed"></iframe>

    </div>

    

    <p class="indent">
        While it may not be abundantly clear, if we sift through this matrix, we can see that words like <i>cats</i> and <i>dogs</i> are frequently used by the first two documents and
        that the third and fourth documents frequently use words like <i>travel</i> and <i>Italy</i>. Just based off of these patterns, we can already begin to get a sense of how these
        documents are similar.
    </p>

    <p class="indent">
        By treating the rows of this matrix as vectors (often referred to as <i>document vectors</i>), we can quantify the similarity of two documents based off of their document vector's
        <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>. Cosine similarity simply tells us how large the angle is between any two vectors. If the angle
        between two vectors is small, meaning that they're <i>similar</i>, then the cosine similarity score will be high.
    </p>

    <p class="indent">
        In the code below, I do a quick pairwise cosine similarity calculation between every document vector in our collection to get an idea of how similar each document is to
        the others:
    </p>

<pre><code class="Python code_output">from sklearn.metrics.pairwise import cosine_similarity

num_documents = frequency_matrix.toarray().shape[0]

print("Cosine Similarity: \n")
for i in range(num_documents-1):
    for j in range(i+1, num_documents):
        print("\t Document %d and %d: %s" % 
        (i+1, j+1, str(cosine_similarity(frequency_matrix[i], frequency_matrix[j])).strip('[]')))
</code></pre>

<pre><code class="nohighlight code_output">Output:

Cosine Similarity:

    Document 1 and 2: 0.30555556
    Document 1 and 3: 0.07273930
    Document 1 and 4: 0.10206207
    Document 2 and 3: 0.24246432
    Document 1 and 4: 0.13608276
    Document 3 and 4: 0.53452248
</code></pre>

    <p class="indent">
        As we can see, the document pairs (1,2) and (3,4) have the highest cosine similarity scores, which is what we would expect to see from these documents, but we can also see that 
        the document pair (2,3) has an unexpectedly high similarity score.
    </p>

    <h2>Stop Words and Tf-idf</h2>

    <p class="indent">
        The reason why this pair has a high cosine similarity score is due to the fact that both of these documents use the word "to" frequently. This points out a major flaw in
        this model. By simply letting word frequencies contribute to the similarity between documents, we are allowing for certain, not very useful words, to contribute to the similarity.
    </p>

    <p class="indent">
        For example, the word "and" appears in nearly every English body of text that you can come across. If you let "and" contribute to the similarity, then almost every body of text 
        is going to be related since almost all of them contain the word "and." 
    </p>

    <p class="indent">
        Due to this fact, before we make our frequency matrix, it is common practice to use a <a href="https://en.wikipedia.org/wiki/Stop_words">stop list</a> to filter out conjuctions, 
        prepositions and determiners since they appear in the majority of documents and do not contribute all that much information to the subject of the document.
    </p>

    <p class="indent">
        To further increase the accuracy of our cosine similarity scores, we introduce a weighting scheme known as <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> to 
        weight each word based off of how frequently it appears and the number of documents that it appears in. Doing this in some sense quantifies the "importance" of a word to a 
        collection of documents. If a word appears infrequently in a select few documents, then it is likely that the word is a defining feature to the documents and dictates the subject.
    </p>

    <p class="indent">
        After removing our stop words and running the tf-idf weighting scheme over our collection of documents, we produce the <i>tf-idf</i> matrix:
    </p>

    <div class="heatmap">

        <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/9.embed"></iframe>

    </div>

    

    <p class="indent">
        After removing stop words and introducing the tf-idf weighting scheme, the similarities between the documents becomes much more clear. Now we are really able to pick up
        on the fact that, for example, words like <i>cats</i>, <i>dogs</i> and <i>pets</i> are prominent words that appear frequently inside of the first two documents.
    </p>

    <p class="indent">
        Let's take a look at the cosine similarities between these new document vectors:
    </p>

<pre><code class="nohighlight code_output">Output:
            
Cosine Similarity:

    Document 1 and 2: 0.26221910
    Document 1 and 3: 0.04448158
    Document 1 and 4: 0.05626396
    Document 2 and 3: 0.14167618
    Document 1 and 4: 0.07168148
    Document 3 and 4: 0.39328159
</code></pre>

    <p class="indent">
        As we can see, the similarity scores for all of the document pairs have gone down, but some have been squashed more than others. In paticular, the documents that we know
        are not similar have been, for the most part, driven down to less than 20%.
    </p>

    <h2>Singular Value Deomposition and Semantic Space</h2>

    <p class="indent">
        The tf-idf model coupled with cosine similarity provides us with a great approximation for the similarity between documents, but there are some limitations to this model.
    </p>

    <p class="indent">
        First of all, this model compares documents based on the idea that documents that frequently use the same words will be similar to one another. While for the most part, this works
        decently well, the problem with this idea is that it leads to the inability to pick up on <i>synonyms</i>. Suppose, for example, we chose to add a fifth document to our collection 
        that frequently used the words <i>feline</i> and <i>canidae</i>. The tf-idf model would fail to recognize that the first two documents are similar to this new one since neither of 
        them use the word feline!
    </p>

    <p class="indent">
        The second issue with this model boils down to the fact that in any real world situation, we would be using this technique to analyze <i>thousands</i> of documents that contain 
        <i>tens of thousands</i> of words, which leads to document vectors that live in a very high dimensional space. Cosine similarity is only useful if the vectors that we're studying 
        live in a relatively low dimensional space. As we increase the dimension that our vectors live in, the angle between any two of our vectors gets smaller, hence, cosine similarity 
        is useless since in very high dimensions, every vector is going to appear to be similar. This is known as the 
        <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>.
    </p>


    <p class="indent">
        To counteract these issues, we need to find a way to pick up on synonyms and also reduce the dimension of these document vectors. Conveniently, this can be done 
        with <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">singular value decomposition</a>.
        
        of our tf-idf matrix and then using this decomposition to project our documents into <a href="https://en.wikipedia.org/wiki/Semantic_space">
        semantic space</a>.
    </p>

    <p class="indent">
        Singular value decomposition provides us with a way to decompose our tf-idf matrix into a product of three smaller matrices:
    </p>

    <p class="indent">
        Given an arbitrary <i>transposed</i> tf-idf matrix

                <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>D</mi>
                  </math>, 
                  there exists a singular value decomposition of the form:

                  <div class="math_block">

                        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                            <mi>D</mi> <mo>=</mo> <mi>U</mi> <mi>S</mi><msup><mi>V</mi><mn>T</mn></msup>
                        </math>

                  </div>
                  

            
            
    </p>
    <p class="indent">
        where 
        <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <mi>U</mi> </math> is a unitary matrix that describes how are the <i>words in our documents</i> are related to our 
        <i>features</i> (I'll explain what I mean by this in just a second), <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <mi>S</mi> </math> describes the <i>strength</i> of our features and 
        <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <msup><mi>V</mi><mn>T</mn></msup> </math> describes how our <i>documents</i> are related to our features.
    </p>

    <p class="indent">
        What's a feature you might ask? When we initially started this example, we intuitively divided our collection of documents into two categories: <i>pets</i> and <i>travel</i>.
        These categories are examples of <i>features</i> for our collection of documents.
    </p>

    <p class="indent">
        Here is the <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <mi>U</mi> </math> matrix for our collection of documents:
    </p>

    <div class="heatmap">
        <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/4.embed"></iframe>
    </div>

    <p class="indent">
        As we can see, words like <i>cats</i> and <i>dogs</i> are strongly associated with our first feature: <i>pets!</i> Furthermore, words like <i>travel</i> and <i>Italy</i>
        are strongly associated with our second feature: <i>travel!</i> 
    </p>

    <p class="indent">
        It's unclear to me what the third and fourth features are trying to describe. By taking a quick look at our <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <mi>S</mi> </math> 
        matrix, which is a diagonal matrix that consists of singular values that describe the strength of each of our features:
    </p>

<pre><code class="nohighlight code_output">Singular Values:

    1.225233566301
    1.077347205740
    0.863096120569
    0.770188803448
</code></pre>

    <p class="indent">
        We can see that our first two features are the dominating features in our collection of documents.
    </p>

    <p class="indent">
        Lets take a look at our <math class="math_font_inline" xmlns="http://www.w3.org/1998/Math/MathML"> <msup><mi>V</mi><mn>T</mn></msup> </math> matrix:
    </p>

    <div  class="heatmap" align= "center">
        <iframe width="475" height="400" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/6.embed"></iframe>
    </div>

    <p class="indent">
        As we can see, the first two documents are highly associated with our first feature, and the third and fourth documents are highly associated with our second feature, as we would
        expect.
    </p>

    <p class="indent">
        As we can see, the decomposition allows us to pick up synonyms, but the real guts of this method comes from projecting our matrix into semantic space.
    </p>
    

</div>