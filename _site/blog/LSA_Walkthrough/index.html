<!DOCTYPE html>
<html>
    <head>
        <title>Latent Semantic Analysis</title>
    
        <link rel="stylesheet" type="text/css" href="/css/main.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  
    </head>


    <body>

        <nav>

            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="/projects">Projects</a></li>
                <li><a href="/blog">Blog</a></li>
            </ul>

        </nav>

        <div class="container">

            <div class="main">
    <h2>Latent Semantic Analysis</h2>

    <blockquote style="text-indent: 3em">
        Within the realm of natural language processing, <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a> (LSA) is a tool that we use to 
        analyze the relationships between a set of documents and the words that they contain. This post will take an in-depth look at LSA and show how it can be used to sort a collection
        of documents (a <i>corpus</i>) into groupings of similar documents.
    </blockquote>

    <blockquote style="text-indent: 3em">
        Repository for the Python code that was used in this post: <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Example">LSA Walkthrough</a>
        <br/><br/>
    </blockquote>


    <blockquote style="text-indent: 3em">
        Consider the following four documents:
    </blockquote>


    <img src="\pictures\Documents.svg" title="Example Documents" class="center_image"/>

    <blockquote style="text-indent: 3em">
        After reading through these documents, it’s easy to see that they could be sorted into two different categories. For the sake of simplicity, let’s call these categories 
        <i>pets</i> and <i>travel</i>. 
    </blockquote>

    <blockquote style="text-indent: 3em">
        This is a rather trivial conclusion for us to draw since there is a clear difference in subject between the first two documents and the last two documents. However, for a 
        computer, identifying the differences is not a simple task. What techniques do we have at our disposal to make a computer to recognize these kinds of differences?
    </blockquote>

    <blockquote style="text-indent: 3em">
        One common techinque starts by looking at the frequencies at which words appear inside of each document. Intuitively, by grouping together the documents that 
        use the same words frequently, we can naively sort the documents. For example, the first two documents use the words <i>cats</i> and <i>dogs</i> frequently while the other 
        two documents do not, hence it may be reasonable to assume that these two documents are similar to each other and can be placed in the same category.
    </blockquote>

    <blockquote style="text-indent: 3em">
        To quantify this idea, it is common practice to convert a corpus into what is known as a <i>document-term</i> frequency matrix. Each row of this matrix represents a document 
        and the columns represent the words that appear inside of these documents. By letting each entry of this matrix represent the frequency at which these words 
        appear inside of each document, we have a consise way of representing these documents based off of their word frequencies. The rows of this matrix are often referred to as 
        <i>document vectors</i>.
    </blockquote>

    <blockquote style="text-indent: 3em">
        Here is the document-term frequency matrix for our example documents (note that I filtered out useless words like <i>and</i> and <i>the</i> before I created the matrix by using a 
        <a href="https://en.wikipedia.org/wiki/Stop_words">stop list</a>):
    </blockquote>

    <iframe width="1400" height="350" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/2.embed"></iframe>

    <blockquote style="text-indent: 3em">

        I converted the matrix into a heat map using R where dark blue represents a word that appears at a high frequency inside of a document. With this visualization, we can see not only what words
        appeared most frequently inside of each of these documents, but we can also see which documents used the same words together at the highest frequency.
        
    </blockquote>

    <blockquote style="text-indent: 3em">
        While it may not be abundantly clear which documents belong to the same category based off of the document-term frequency matrix, we can certainly begin to see correlations between
        the documents based off which documents use the same words.
    </blockquote>

    <blockquote style="text-indent: 3em">
        
    </blockquote>

    <iframe width="1400" height="350" frameborder="0" scrolling="no" src="//plot.ly/~BrysonSeiler/4.embed"></iframe>

    <blockquote style="text-indent: 3em">
        While it may not be abundantly clear which documents belong to the same category based off of the document-term frequency matrix, we can certainly begin to see correlations between
        the documents based off which documents use the same words.
    </blockquote>
    

</div>

        </div>

        <footer>
            <ul>
                <li><a href="Bryson.Seiler@gmail.com">Email</a></li>
                <li><a href="https://github.com/BrysonSeiler">GitHub</a></li>
                <li><a href="https://www.linkedin.com/in/brysonseiler/">LinkedIn</a></li>
            </ul>

            <inf_text>
                
                 This website was influenced by <a href="http://jmcglone.com/guides/github-pages/"> Jonathan McGlone's guide </a> to hosting a personal site on GitHub.

            </inf_text>

        </footer>

        
        
    </body>

</html>