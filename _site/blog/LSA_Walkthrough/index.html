<!DOCTYPE html>
<html>

<head>
    <title>Latent Semantic Analysis</title>

    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">

</head>


<body>

    <nav>

        <ul>
            <li>
                <a href="/">Home</a>
            </li>
            <li>
                <a href="/projects">Projects</a>
            </li>
            <li>
                <a href="/blog">Blog</a>
            </li>
            <li>
                <a href="/CV">CV</a>
            </li>
        </ul>

    </nav>

    <div class="container">

        <div class="main">
    <h2>Latent Semantic Analysis</h2>

    <p class="indent">
        Within the realm of natural language processing,
        <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a> (LSA) is a tool that is used to analyze the relationships between a set of documents
        based on the words that they contain. This post will take an in-depth look at LSA and show how it can be used to sort
        large collections of documents.
    </p>

    <p class="indent">
        The Python code that was used in this post can be found here:
        <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Example">LSA Walkthrough</a>
        <br/>
    </p>

    <p class="indent">
        To understand Latent Semantic Analysis, lets walk through an example of how it can be used to sort the following four (very unrealistic) documents:
    </p>

    <div class="center">
        <img src="\pictures\Documents.svg" title="Example Documents"/>
    </div>
    

    <p class="indent">
        After reading through these documents, it’s not hard to see that they could be sorted into two different categories. For the
        sake of simplicity, let’s call these categories
        <i>pets</i> and
        <i>travel</i>.
    </p>

    <p class="indent">
        This is a rather trivial conclusion for us to draw since the subject of the first two documents is clearly different than the last two, but for a computer,
        identifying these subject differences is no simple task. What techniques do we have at our disposal to make a computer to recognize these kinds of differences?
    </p>

    <h3>Document-Term Frequency</h3>

    <p class="indent">
        A common starting point begins by looking at the frequencies at which the words inside of a document appear inside of the others. Intuitively, by grouping together
        the documents that use the same words frequently, we can (naively) sort the documents. To do this, it is common practice to convert a collection of documents into what 
        is known as a <i>document-term</i> frequency matrix (in my code and from here on out, I will refer to it as a "frequency matrix").
    </p>


    <p class="indent">
        Each row of this matrix represents a document and the columns represent the words
        that appear inside of these documents. By letting each entry of this matrix represent the <i>frequency</i> at which these
        words appear inside of each document, we have a consise way of representing these documents based off of their word
        frequencies.
    </p>

    <p class="indent">
        Here is the document-term frequency matrix represented as a heat map for our example documents:
    </p>

    <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/2.embed"></iframe>

    <p class="indent">
        While it may not be abundantly clear, if we sift through this matrix, we can see that words like <i>cats</i> and <i>dogs</i> are frequently used by the first two documents and
        that the third and fourth documents frequently use words like <i>travel</i> and <i>Italy</i>. Just based off of these patterns, we can already begin to get a sense of how these
        documents are similar.
    </p>

    <p class="indent">
        By treating the rows of this matrix as vectors (often referred to as <i>document vectors</i>), we can quantify the similarity of two documents based off of their document vector's
        <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>. Cosine similarity simply tells us how large the angle is between any two vectors. If the angle
        between two vectors is small, meaning that they're <i>similar</i>, then the cosine similarity score will be high.
    </p>

    <p class="indent">
        In the code below, I do a quick pairwise cosine similarity calculation between every document vector in our collection to get an idea of how similar each document is to
        the others:
    </p>
    
    <div class="center">
        <img src="\pictures\LSA_Cosine_Count.png" title="LSA Cosine Similarity"/>
    </div>

    <p class="indent">
        As we can see, the document pairs (1,2) and (3,4) have the highest cosine similarity scores, which is what we would expect to see from these documents, but we can also see that 
        the document pair (2,3) has an unexpectedly high similarity score.
    </p>

    <h3>Stop Words and Tf-idf</h3>

    <p class="indent">
        The reason why this pair has a high cosine similarity score is due to the fact that both of these documents use the word "to" frequently. This points out a major flaw in
        this model. By simply letting word frequencies contribute to the similarity between documents, we are allowing for certain, not very useful words, to contribute to the similarity.
    </p>

    <p class="indent">
        For example, the word "and" appears in nearly every English body of text that you can come across. If you let "and" contribute to the similarity, then almost every body of text 
        is going to be related since almost all of them contain the word "and." 
    </p>

    <p class="indent">
        Due to this fact, before we make our frequency matrix, it is common practice to use a <a href="https://en.wikipedia.org/wiki/Stop_words">stop list</a> to filter out conjuctions, 
        prepositions and determiners since they appear in the majority of documents and do not contribute all that much information to the subject of the document.
    </p>

    <p class="indent">
        To further increase the accuracy of our cosine similarity scores, we introduce a weighting scheme known as <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> to 
        weight each word based off of how frequently it appears and the number of documents that it appears in. Doing this in some sense quantifies the "importance" of a word to a 
        collection of documents. If a word appears infrequently in a select few documents, then it is likely that the word is a defining feature to the documents and dictates the subject.
    </p>

    <p class="indent">
        After removing our stop words and running the tf-idf weighting scheme over our collection of documents, we produce the <i>tf-idf</i> matrix:
    </p>

    <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/9.embed"></iframe>

    <p class="indent">
        After removing stop words and introducing the tf-idf weighting scheme, the similarities between the documents becomes much more clear. Now we are really able to pick up
        on the fact that, for example, words like <i>cats</i>, <i>dogs</i> and <i>pets</i> are prominent words that appear frequently inside of the first two documents.
    </p>

    <p class="indent">
        Let's take a look at the cosine similarities between these new document vectors:
    </p>

    <div class="center">
        <img src="\pictures\LSA_Cosine_Tfidf.png" title="LSA Cosine Similarity"/>
    </div>

    <p class="indent">
        As we can see, the similarity scores for all of the document pairs have gone down, but some have been squashed more than others. In paticular, the documents that we know
        are not similar have been, for the most part, driven down to less than 20%.
    </p>

    <h3>Singular Value Deomposition and Semantic Space</h3>

    <p class="indent">
        Cosine similarity is useful if the vectors that we're studying live in a relatively low dimension. As we increase the dimension that our vectors live in, the angle between
        any two of our vectors gets smaller. Hence, cosine similarity is useless since in very high dimensions, every vector is going to appear to be similar.
        
        This is known as the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>.
        
        
        The reality of LSA is that we are almost always going 
        to be analyzing thousands of documents with tens of thousands of words, which implies that our document vectors are going to live in a very high dimensional space. 
    </p>

    <iframe width="100%" height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/4.embed"></iframe>

    <p class="indent">
        
    </p>

    <iframe height="350" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/6.embed" class="center_heatmap"></iframe>

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>
    When
    <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>a</mi><mo>&#x2260;</mo><mn>0</mn>
    </math>,
    there are two solutions to
    <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>a</mi><msup><mi>x</mi><mn>2</mn></msup>
      <mo>+</mo> <mi>b</mi><mi>x</mi>
      <mo>+</mo> <mi>c</mi> <mo>=</mo> <mn>0</mn>
    </math>
    and they are
    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
      <mi>x</mi> <mo>=</mo>
      <mrow>
        <mfrac>
          <mrow>
            <mo>&#x2212;</mo>
            <mi>b</mi>
            <mo>&#x00B1;</mo>
            <msqrt>
              <msup><mi>b</mi><mn>2</mn></msup>
              <mo>&#x2212;</mo>
              <mn>4</mn><mi>a</mi><mi>c</mi>
            </msqrt>
          </mrow>
          <mrow> <mn>2</mn><mi>a</mi> </mrow>
        </mfrac>
      </mrow>
      <mtext>.</mtext>
    </math>
    </p>


</div>

    </div>

    <footer>
        <ul>
            <li>
                <a href="Bryson.Seiler@gmail.com">Email</a>
            </li>
            <li>
                <a href="https://github.com/BrysonSeiler">GitHub</a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/brysonseiler/">LinkedIn</a>
            </li>
        </ul>

        <p class="footnote">

            This website was influenced by
            <a href="http://jmcglone.com/guides/github-pages/"> Jonathan McGlone's guide </a> to hosting a personal site on GitHub.

        </p>

    </footer>



</body>

</html>