<!DOCTYPE html>
<html>

<head>
    <title>Subreddit Analysis</title>

    <link rel="stylesheet" type="text/css" href="/css/main.css">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Lato:700|Raleway:500,600|Roboto:700" rel="stylesheet">

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/docco.min.css">
    
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

    <script>hljs.initHighlightingOnLoad();</script>
    
</head>

<body class = "body_text">

    <div class="navigation">
        <a href="/">Home</a>
        <a href="/projects">Projects</a>
        <a href="/blog">Blog</a>
        <a href="/CV">CV</a>
        <a href="/References">References</a>
    </div>

    <div class="container">
        <div class="main">
    <h1>Subreddit Analysis</h1>

    <p class="indent">
        If you aren’t familiar with <a href="https://www.reddit.com">Reddit</a>, it is a bulletin-board style website where you can post/share interesting links, pictures, thoughts, 
        etc. with others. To organize what you share, Reddit uses <i>subreddits</i>, which are forums that are dedicated to a specific topic on the website. From science, politicst to
        you name it, there is probably a subreddit for it.
    </p>

    <p class="indent">
        If you spend enough time on Reddit, you may notice that the posts within a subreddit typically use a similar vocabulary. For example, posts within the personal finance subreddit are more likely to use words like
        “money”, “debt”, and “loans” than most other subreddits. This raises some interesting questions:
    </p>

    <blockquote class="reference">
        <p>
            <i>Which subreddits have the most similar vocabulary? In other words, which subreddits are semantically similar/dissimilar?</i> <br/><br/>
            <i>Given a post, can we make an educated guess as to which subreddit it came from?</i>
        </p>
    </blockquote>

    <p class="indent">
        These are questions that can be answered with Latent Semantic Analysis (LSA). If you are unfamiliar with LSA, check out my <a href="/blog/LSA_Walkthrough/">blog post</a> 
        on it. I spent some time trying to flesh out the core of how it works.
    </p>

    <p class="indent">
        In theory, if we were to gather a bunch of post titles (which I will now refer to as submissions) from different subreddits, we could then use LSA to extract the 
        semantic similarity between them. Let's see if it works...
    </p>

    <p class="indent">
        The Python code for this project can be found here:
        <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Reddit">Reddit Latent Semantic Analysis</a>
        <br/>
    </p>

    <h2>Gathering and Processing Submission Titles From Reddit</h2>

    <p class="indent">
        To gather submissions from Reddit, I used the Python package <a href="https://praw.readthedocs.io/en/latest/">PRAW</a> (Python Reddit API Wrapper). PRAW is an excellent package
        that allows for simple access to Reddit's API. It's easy to use and it allows you to gather up to 1000 submissions (per subreddit) at once. 
    </p>

    <p class="indent">
        Once you have signed up to access Reddit's API and have your credientials for OAuth2, we can have access to subreddit submissions with only a few lines of code. Here's the
        general idea:
    </p>

<pre><code class="Python code_highlight">#Use your credentials to log into your bot's account
reddit_bot.login()
                
#Create a subreddit object for a specific subreddit that contains all provided attributes of that subreddit
subreddit = reddit_bot.subreddit(subreddit_name)
                
#Print top level comments from x number of submissions inside of the subreddit
for submission in subreddit.top(limit = num_submissions):
    print(submission)
            
</code></pre>

    <p class="indent">
        For this project, I decided to keep it simple gather 1000 submissions from three different subreddits: Westworld, Doctor Who, and Game of Thrones. These are T.V. show fan base subreddits where character 
        names will be frequently mentioned. My hope is that LSA will pick up on this fact and be able to differentiate betwen the subreddits.
    </p>

    <p class="indent">
        While gathering submissions from each subreddit, I filter out all of the special characters along with stop words and remove capitalization to make it easier for the data to be processed when it comes to 
        applying LSA. By using <code class="code_inline">stop_words = set(list(text.ENGLISH_STOP_WORDS) +  list(stopwords.words("english")))</code> I was able to combine both the <code class="code_inline">nltk</code> and 
        <code class="code_inline">sklearn</code> stop word sets since individually, they did not seem to remove the breadth of stop words that I would have liked. Here's some sample output showing before/after 
        the filtering of the submisisons:
    </p>

<pre><code class="nohighlight code_output">Gathering top 3 submission titles from westworld...

Submission title: My Secret Santa sent me a package, but it doesn't look like anything to me.

Filtered submission: secret santa sent package look like

Submission title: Everything you feel is true. Ed Harris has been nominated for “Outstanding Lead Actor in a Drama Series” at this year’s Emmys.

Filtered submission: feel true ed harris nominated outstanding lead actor drama series year emmys

Submission title: Reading the Emmy nominations and not seeing anything for the true star of season 2.

Filtered submission: reading emmy nominations seeing true star season

Gathered 3 submissions out of 3 --- 100.00%
</code></pre>

    <p class="indent">
        After trying to gather 1000 submissions from each subreddit, I was able to successfully gather 2947 of the 3000 submissions (98.63%). This is due to the fact that some of the submissions were
        either empty or deleted. After gathering the submissions, I then put all 2947 submissions into a pandas data frame (what I refer to in my code as bundling) and then 
    </p>

<pre><code class="nohighlight code_output">Gathering top 1000 submission titles from westworld...
Gathered 974 submissions out of 1000 --- 97.40%
            
Gathering top 1000 submission titles from doctorwho...
Gathered 998 submissions out of 1000 --- 99.80%
            
Gathering top 1000 submission titles from gameofthrones...
Gathered 987 submissions out of 1000 --- 98.70%
            
Gathered a total of 2959 submission titles over 3 subreddits --- 98.63%
            
Bundling up submissions...
            
Successfully bundled: 2947 submissions
Successfully tagged: 2947 submissions
</code></pre>


    <p class="indent">
        When I refer to tagging and bundling, I am refering to the fact that I bundled all 2947 submissions into one pandas data frame and then created a separate column to tag each of the submissions with the proper subreddit
        in which the submission came from. This will be the raw data that is used when we run LSA.
    </p>


    <p class="indent">
        Now that I have successfully filtered, bundled and tagged the submissions, its time to do some initial analyis on this data by converting the raw data into their frequency and tfidf matrix representations. To
        do so, I used <code class="code_inline">sklearn</code>. The general idea to convert a raw text collection into its freqency or tfidf repesenation is rather straight-forward.
    </p>

<pre><code class="Python code_highlight">def get_frequency_matrix(raw_text):

    #Convert our raw text into a frequency matrix
    count_vectorizer = CountVectorizer(max_df=0.25, max_features=500)
    frequency_matrix = count_vectorizer.fit_transform(raw_text)

    #Get the feature names from our 
    feature_names = count_vectorizer.get_feature_names()
    frequency_df = pd.DataFrame(frequency_matrix.toarray(), columns=feature_names)

    return frequency_df, feature_names

def get_tfidf_matrix(feature_names, frequency_df):

    tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True)
    tfidf_matrix = tfidf_transformer.fit_transform(frequency_df)
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

    return tfidf_df
            
</code></pre>


<pre><code class="nohighlight code_output">Learning vocabulary...

Building frequency matrix...                                                                        Building tf-idf matrix...
Frequency matrix shape: 2947 by 500                                                                 Tfidf matrix shape: 2947 by 500

Top 10 most frequently occuring terms in each subreddit (frequency, word):                          Top 10 highest tfidf scoring terms in each subreddit (cumulative score, word):
            
          westworld      doctorwho   gameofthrones                                                                     westworld                 doctorwho             gameofthrones
0  (232, westworld)  (228, doctor)    (84, season)                                                  0  (97.213372539, westworld)   (101.560813588, doctor)     (35.2739945794, game)
1     (110, season)   (70, tardis)      (83, game)                                                  1    (44.4450434939, season)   (38.3042412892, tardis)  (35.0017312287, thrones)
2     (81, episode)    (53, today)   (78, thrones)                                                  2   (31.3725706244, episode)    (23.2506873032, today)   (34.9305434273, season)
3        (43, like)      (43, got)   (63, episode)                                                  3   (24.8068474103, dolores)   (20.482284365, cosplay)  (26.1464311832, episode)
4     (41, dolores)       (39, th)       (49, got)                                                  4      (23.4631433028, ford)       (18.3196733655, th)      (22.5882265291, jon)
5        (39, ford)      (39, new)       (44, jon)                                                  5      (18.6375845419, like)      (17.9412010307, new)      (20.9237859057, got)
6        (31, post)  (38, cosplay)  (32, daenerys)                                                  6   (18.2665834166, bernard)      (16.4741964929, got)  (20.465286075, daenerys)
7       (30, black)   (33, friend)   (29, cosplay)                                                  7      (15.2148709101, post)      (15.8375799748, met)     (14.7023527661, snow)
8     (30, bernard)     (32, matt)      (26, snow)                                                  8      (14.3639195808, host)  (15.4155473247, doctors)  (14.6150241288, cosplay)
9       (25, scene)      (32, met)      (26, like)                                                  9     (13.9498082193, maeve)   (15.3802164488, friend)     (14.1079479072, king)
            
</code></pre>

    <p class="indent">
        From the frequency matrix, we can see what the most commonly occuring words for each subreddit are.
    </p>

     

    


    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>
<pre><code class="nohighlight code_output">Reducing dimension of data...
Successfully reduced data to 2947 submissions explained by 300 features
Reduced matrix shape: 2947 by 300
            
Explained variance of first 5 components:  0.01797551  0.02242872  0.01745798  0.0132853   0.01079801
            
Singular values:  8.0255198   7.77709256  6.80748527  5.99201428  5.40177308
            
Percent variance explained by all 300 components: 0.848
</code></pre>

    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>

    <iframe width="100%" height="800" margin-left: "auto" margin-right: "auto" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/18.embed"></iframe>

<pre><code class="nohighlight code_output">Distance metric:

    1. Euclidean
    2. Cosine (Slow on large batches)

Choose metric: 1

Running k-means using euclidean distance...

Successfully found 3 clusters in 300 dimensions

Successfully grouped 71.3% of submissions
</code></pre>

<p>
    Hm. Okay, well it did alright, but not perfect. It seems like a large ammount of the data clusters into that corner. 
</p>

</div>
    </div>

</body>

<div class="footer">
    <div class="footer-top">
        <a href="Bryson.Seiler@gmail.com">Email</a>
        <a href="https://github.com/BrysonSeiler">GitHub</a>
        <a href="https://www.linkedin.com/in/brysonseiler/">LinkedIn</a> 
    </div>
    <div class="footer-bottom">
        <p>
            This website was influenced by
            <a href="http://jmcglone.com/guides/github-pages/"> Jonathan McGlone's guide </a> to hosting a personal site on GitHub. <br/>
            Code highlighting by <a href="https://highlightjs.org/"> Highlight.js</a> (<a href="https://github.com/highlightjs/highlight.js/blob/master/LICENSE">BSD license</a>).<br/>
            <a href="https://fonts.google.com/"> Google Fonts</a>: Roboto, Fira Mono.
        </p>
    </div>
</div>

</html>