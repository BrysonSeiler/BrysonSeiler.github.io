<!DOCTYPE html>
<html>

<head>
    <title>Subreddit Analysis</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Lato:700|Roboto:300" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-sulphurpool-light.min.css">
    
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
</head>

<body class = "page_layout">

    <div class="navigation">
        <a href="/">Home</a>
        <a href="/projects">Projects</a>
        <a href="/blog">Blog</a>
        <a href="/CV">CV</a>
        <a href="/References">References</a>
    </div>

    <div class="container">
        <div class="main">
    <h1>Subreddit Analysis</h1>

    <div class="abstract">
        <i class="indent">
            <b>Abstract - </b> The purpose of this project is to analyze the relationships between communities on the website
            Reddit based on the unique words used within each subreddit. Latent Semantic Analysis will be used to analyze these
            relationships and to identify similar communities.
        </i>
    </div>

    <div class="header_block">
        <h2>Introduction</h2>
    </div>
    
    <div>
        <p class="indent">
            If you aren’t familiar with <a href="https://www.reddit.com">Reddit</a>, it is a bulletin-board style website where
            you can share interesting links, pictures, etc. with others. To organize what you share on the website, Reddit asks
            you to choose a specific <i>subreddit</i> to post your information to. A subreddit is just a forum on the website
            that is dedicated to a specific topic. For example, if you wanted to share pictures from your backpacking trip along
            the Appalachian Trail, the most obvious subreddit to choose would be <a href="https://www.reddit.com/r/hiking/">r/hiking</a>,
            but there’s an even more specific subreddit, <a href="https://www.reddit.com/r/AppalachianTrail/">r/AppalachianTrail</a>,
            that would probably be a better choice. There is a subreddit for just about anything that you can think of. Whether
            you’re interested in science, politics, economics, etc., there is probably a subreddit for it.
        </p>
    
        <p class="indent">
            A direct result of having these different subreddit communities is that they have their own unique <i>vocabularies</i>.
            For example, posts within the hiking subreddit are more likely to contain words like “outdoors” and “nature” than
            a subreddit like math where the posts would be more likely to contain words like “algebra” and “calculus.” While
            this may seem obvious, recognizing that each subreddit has its own unique vocabulary helps tease out the idea that
            there is an underlying structure to the website based off each subreddits vocabulary.
        </p>
    
        <p class="indent">
            With this idea in mind, we can start to ask questions like:
        </p>
    
        <div class="thought">
            <i>Which subreddits have the most similar vocabularies? In other words, which subreddits are the most semantically
                similar/dissimilar?</i> <br/>
            <i>Given a random sentence, can I make a prediction as to which subreddit it came from?</i>
        </div>
    
        <p class="indent">
            For this project, I will attempt to answer these questions with Latent Semantic Analysis (LSA). If you are unfamiliar with
            LSA, I highly suggest reading my <a href="/blog/LSA_Walkthrough/">blog post</a> on it. I spent a good amount
            of time trying to flesh out the core ideas behind how it works.
        </p>
    
        <p class="indent">
            The Python code for this project can be found <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Reddit">here</a>.
            <br/>
        </p>
    </div>

    <div class="section_break"></div>

    <div class="header_block">
        <h2>Methodology</h2>
    </div>

    <div>
        <p class="indent">
            To analyze the differences/similarities between subreddits, I will gather post titles from different subreddits and then
            use Latent Semantic Analysis and k-means clustering to create clusters of post titles that are semantically similar.
        </p>
    </div>

    

    <div class="header_block">
        <h3>Outline</h3>
    </div>

    <div class="ordered_list">
        <ol type="1">
            <li>Gather post titles using the Python Reddit API Wrapper package (PRAW).
                <ol type="i">
                    <li>Process post titles by removing special characters and stop words.</li>
                    <li>Bundle together all post titles and tag based off origin subreddit.</li>
                </ol>
            </li>
            <li>Produce frequency/tfidf matrices and then project tfidf matrix to semantic space.
                <ol type="i">
                    <li>Use bundled post titles as raw data set.</li>
                    <li>Use <code class="code_inline">sklearn</code> to produce matrices.</li>
                </ol>
            </li>
            <li>Cluster post titles with k-means clustering.
                <ol type="i">
                    <li>Use <code class="code_inline">sklearn</code> to cluster post titles in semantic space.</li>
                    <li>Produce purity measurements of each cluster.</li>
                </ol>
            </li>
        </ol>
    </div>

    <div class="section_break"></div>

    <div class="header_block">
        <h2>Gathering and Processing Post Titles From Reddit</h2>
    </div>
<div>
    <p class="indent">
        To gather post titles from Reddit, I decided to use the Python package <a href="https://praw.readthedocs.io/en/latest/">PRAW</a>
        (Python Reddit API Wrapper) as it provides easy to access to Reddit’s API and ensures that I don’t exceed the API’s
        request rate limits. One downside to Reddit’s API is that I’m limited to gathering 1000 post titles per request,
        but for the time being, if we are gathering 1000 post titles from multiple subreddits, we should have enough data
        to mess around with.
    </p>

    <p class="indent">
        Once you have created your Reddit developer account and have your credentials for OAuth2, we can begin gathering post titles
        from any subreddit of our choosing with only a few of lines of code. Here's an example:
    </p>
</div>

<pre><code class="Python code_output">import config
import praw

num_posts = 5
    
#Use your credentials to log into your bot's account
bot = praw.Reddit(username = config.username,
                password = config.password,
                client_id = config.client_id,
                client_secret = config.client_secret,
                user_agent = "Subreddit_Analysis")
                
#Create a object for the 'hiking' subreddit
subreddit = bot.subreddit('hiking')
                
#Print post titles from x number of posts inside of the 'hiking' subreddit (can be sorted by top, hot, new, ...)
for post in subreddit.hot(limit = num_posts):
    print(post.title, "\n")
            
</code></pre>

<div>
    <p>
        Output:
    </p>
</div>

<pre><code class="nohighlight code_output">Yesterday, at Chopok Mountain, Slovakia [OC]

2,917 meters above sea level with view to the „Gschnitzer and Pflerscher Tribulaun“, Schwarze Wand - Obernberg valley, Tirol, Austria
        
Atop Grassy Ridge Bald, Pisgah National Forest, N.C./Tenn. boarder, U.S.A.
        
Hiking to Island Lake, Colorado is well worth the long drive!
        
I love it when I get to cross a beautiful footbridge on the trail. Caples Creek, Ca.         
</code></pre>

<div>
    <p class="indent">
        As you can see, it is rather easy to start gathering post titles from a specific subreddit, but these titles require some
        further processing if we are going to eventually use them for latent semantic analysis. First, we need to remove
        all the special characters, numbers and capitalization. This can be accomplished with regular expressions, specifically,
        the <code class="code_inline">re</code> package in Python.
    </p>

    <p class="indent">
        Continuing off of the code above, we can quickly implement a method to clean up the titles with <code class="code_inline">re</code>:
    </p>
</div>

<pre><code class="Python code_output">import re

def clean(post):

    #Remove parentheses
    re_p = re.sub("([\(\[]).*?([\)\]])", "\g<1>\g<2>", post)

    #Remove links
    re_l = re.sub(r"http\S+|([\(\[]).*?([\)\]])", "", re_p)

    #Remove special characters
    re_s = re.sub(r"[^A-Za-z \—]+", " ", re_l)

    #Remove capitalization
    re_c = re.sub('[A-Z]+', lambda x: x.group(0).lower(), re_s)

    #Remove excess white space
    filtered_post = " ".join(re_c.split())

    return filtered_post
                  
#Print post titles from x number of posts inside of the 'hiking' subreddit (can be sorted by top, hot, new, ...)
for post in subreddit.hot(limit = num_posts):
    print(clean(post.title), "\n")                   
</code></pre>

<div>
    <p class="indent">
        After running our post titles through this method, the following output is produced:
    </p>
</div>

<pre><code class="nohighlight code_output">yesterday at chopok mountain slovakia

meters above sea level with view to the gschnitzer and pflerscher tribulaun schwarze wand obernberg valley tirol austria
        
hiking to island lake colorado is well worth the long drive
        
atop grassy ridge bald pisgah national forest n c tenn boarder u s a
        
i love it when i get to cross a beautiful footbridge on the trail caples creek ca        
</code></pre>

<div>
    <p class="indent">
        Now that we have removed special characters, numbers and capitalization, the titles are much easier to deal with, but
        there is still a couple of issues that need to be dealt with. For example, take a look at this title:
    </p>
</div>

<pre><code class="nohighlight code_output">atop grassy ridge bald pisgah national forest n c tenn boarder u s a</code></pre>

<div>
    <p class="indent">
        As we can see, when we removed the special characters from “U.S.A,” whitespace was left behind. We obviously know that the
        letters “u” and “s” should not be interpreted as words, but sadly later on when we further analyze these titles,
        letters that are separated by whitespace will be interpreted as unique words by the program. Another issue is that
        words that appear frequently like “is” and “the” in our post titles will end up contributing to the noise in our
        data (I further flesh out why this is the case in my <a href="/blog/LSA_Walkthrough/">blog post on LSA</a>).
    </p>

    <p class="indent">
        Both of these problems can by solved by introducing a list of stop words. There are many different websites that can provide
        custom stop word lists that fit different needs better than others. I found that by combining both the <code
            class="code_inline">nltk</code> and <code class="code_inline">sklearn</code> English stop word lists, I was able
        to remove the majority of noise contributed to nuisance words within post titles on reddit, but this is all a matter
        of preference.
    </p>
</div>


<pre><code class="Python code_output">from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction import text

#Set stop words to english (Uses both sklearn and nltk stopwords)
stop_words = set(list(text.ENGLISH_STOP_WORDS) +  list(stopwords.words("english")))

filtered_post = []
                
#Print post title from x number of posts inside of the 'hiking' subreddit
for post in subreddit.hot(limit = num_posts):

    #Clean the post title and tokenize
    post_tokens = word_tokenize(str(clean(post.title)))

    #Remove stopwords from tokenized comment
    for word in post_tokens:
        if word not in stop_words:
            filtered_post.append(word)

    print("Filtered post: %s \n" % str(' '.join(filtered_post)))
    filtered_post = []   
</code></pre>

<div>
    <p class="indent">
        Output:
    </p>
</div>

<pre><code class="nohighlight code_output">Filtered post: yesterday chopok mountain slovakia

Filtered post: meters sea level view gschnitzer pflerscher tribulaun schwarze wand obernberg valley tirol austria
        
Filtered post: hiking island lake colorado worth long drive
        
Filtered post: atop grassy ridge bald pisgah national forest boarder a

Filtered post: love cross beautiful footbridge trail caples creek
</code></pre>

<div>
    <p class="indent">
        After removing stop words, our titles are finally at a point where I feel comfortable feeding them in as data to analyze. 
    </p>
</div>

    <p class="indent">
        After trying to gather 1000 submissions from each subreddit, I was able to successfully gather 2947 of the 3000 submissions (98.63%). This is due to the fact that some of the submissions were
        either empty or deleted. After gathering the submissions, I then put all 2947 submissions into a pandas data frame (what I refer to in my code as bundling) and then 
    </p>

<pre><code class="nohighlight code_output">
</code></pre>


    <p class="indent">
        When I refer to tagging and bundling, I am refering to the fact that I bundled all 2947 submissions into one pandas data frame and then created a separate column to tag each of the submissions with the proper subreddit
        in which the submission came from. This will be the raw data that is used when we run LSA.
    </p>


    <p class="indent">
        Now that I have successfully filtered, bundled and tagged the submissions, its time to do some initial analyis on this data by converting the raw data into their frequency and tfidf matrix representations. To
        do so, I used <code class="code_inline">sklearn</code>. The general idea to convert a raw text collection into its freqency or tfidf repesenation is rather straight-forward.
    </p>

<pre><code class="Python code_output">def get_frequency_matrix(raw_text):

    #Convert our raw text into a frequency matrix
    count_vectorizer = CountVectorizer(max_df=0.25, max_features=500)
    frequency_matrix = count_vectorizer.fit_transform(raw_text)

    #Get the feature names from our 
    feature_names = count_vectorizer.get_feature_names()
    frequency_df = pd.DataFrame(frequency_matrix.toarray(), columns=feature_names)

    return frequency_df, feature_names

def get_tfidf_matrix(feature_names, frequency_df):

    tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True)
    tfidf_matrix = tfidf_transformer.fit_transform(frequency_df)
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

    return tfidf_df
            
</code></pre>


<pre><code class="nohighlight code_output">
            
</code></pre>

    <p class="indent">
        From the frequency matrix, we can see what the most commonly occuring words for each subreddit are.
    </p>

     

    


    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>
<pre><code class="nohighlight code_output">
</code></pre>

    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>

    

    <div class="graph">
        <iframe width="100%" height="800" margin-left: "auto" margin-right: "auto" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/18.embed"></iframe>
    </div>

<pre><code class="nohighlight code_output">
</code></pre>

<p>
    Hm. Okay, well it did alright, but not perfect. It seems like a large ammount of the data clusters into that corner. 
</p>

</div>
    </div>

    <div class="footer">
        <div class="footer-top">
            <a href="Bryson.Seiler@gmail.com">Email</a>
            <a href="https://github.com/BrysonSeiler">GitHub</a>
            <a href="https://www.linkedin.com/in/brysonseiler/">LinkedIn</a> 
        </div>
        <div class="footer-bottom">
            <p>
                This website was influenced by
                <a href="http://jmcglone.com/guides/github-pages/"> Jonathan McGlone's guide </a> to hosting a personal site on GitHub. <br/>
                Code highlighting by <a href="https://highlightjs.org/"> Highlight.js</a> (<a href="https://github.com/highlightjs/highlight.js/blob/master/LICENSE">BSD license</a>).<br/>
                Graphics by <a href="https://bokehplots.com/pages/citation.html"> Bokeh</a> (<a href="https://github.com/bokeh/bokeh/blob/master/LICENSE.txt">BSD license</a>).<br/>
                <a href="https://fonts.google.com/"> Google Fonts</a>: Roboto, Fira Mono.
            </p>
        </div>
    </div>

</body>

</html>