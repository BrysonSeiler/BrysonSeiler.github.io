<!DOCTYPE html>
<html>

<head>
    <title>Subreddit Analysis</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Lato:700|Roboto:300" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/docco.min.css">
    
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
</head>

<body class = "page_layout">

    <div class="navigation">
        <a href="/">Home</a>
        <a href="/projects">Projects</a>
        <a href="/blog">Blog</a>
        <a href="/CV">CV</a>
        <a href="/References">References</a>
    </div>

    <div class="container">
        <div class="main">
    <h1>Subreddit Analysis</h1>

    <div class="abstract">
        <i class="indent">
            <b>Abstract-</b>The purpose of this project is to analyze the differences/similarities between subreddits on the website Reddit, I will gather post titles from different subreddits and then 
            use Latent Semantic Analysis and k-means clustering to create clusters of post titles that are semantically similar. 
        </i>
    </div>

    <div class="header_block">
        <h2>Introduction</h2>
    </div>

    <div>
        <p class="indent">
                If you aren’t familiar with <a href="https://www.reddit.com">Reddit</a>, it is a bulletin-board style website where you can share interesting links, pictures, etc. with others. To organize what you share 
                on the website, Reddit asks you to choose a specific <i>subreddit</i> to post your information to. A subreddit is just a forum on the website that is dedicated to a specific 
                topic. For example, if you wanted to share pictures from your backpacking trip along the Appalachian Trail, the most obvious subreddit to choose would be <a href="https://www.reddit.com/r/hiking/">r/hiking</a>, but 
                there’s an even more specific subreddit, <a href="https://www.reddit.com/r/AppalachianTrail/">r/AppalachianTrail</a>, that would probably be a better choice. There is a subreddit for just about anything that you can think of. 
                Whether you’re interested in science, politics, economics, etc., there is probably a subreddit for it.
        </p>

        <p class="indent">
            A direct result of having these different subreddit communities is that they have their own unique <i>vocabularies</i>. For example, posts within the hiking subreddit are more 
            likely to contain words like “outdoors” and “nature” than a subreddit like math where the posts would be more likely to contain words like “algebra” and “calculus.” While this may seem obvious, recognizing that each subreddit has its own unique vocabulary helps tease out the idea that there is an underlying structure to the website based 
            off each subreddits vocabulary.
        </p>

        <p class="indent">
            With this idea in mind, we can start to ask questions like:
        </p>

        <div class="thought">
            <i>Which subreddits have the most similar vocabularies? In other words, which subreddits are the most semantically similar/dissimilar?</i> <br/>
            <i>Given a random sentence, can I make a prediction as to which subreddit it came from?</i>
        </div>

        <p class="indent">
            For this project, I will attempt to answer these questions with Latent Semantic Analysis (LSA). If you are unfamiliar with LSA, I highly suggest reading my <a href="/blog/LSA_Walkthrough/">blog post</a> on it. 
            I spent a good amount of time trying to flesh out the core ideas behind how it works. 
        </p>

        <p class="indent">
            The Python code for this project can be found <a href="https://github.com/BrysonSeiler/Latent-Semantic-Analysis-Reddit">here</a>.
            <br/>
        </p>
    </div>

    <div class="section_break"></div>

    <div class="header_block">
        <h2>Methodology</h2>
    </div>

    <div>
        <p class="indent">
            To analyze the differences/similarities between subreddits, I will gather post titles from different subreddits and then use Latent Semantic Analysis and k-means clustering to create 
            clusters of post titles that are semantically similar. 
        </p>
    </div>

    

    <div class="header_block">
        <h3>Outline</h3>
    </div>

    <div class="ordered_list">
        <ol type="1">
            <li>Gather post titles using the Python Reddit API Wrapper package (PRAW).
                <ol type="i">
                    <li>Process post titles by removing special characters and stop words.</li>
                    <li>Bundle together all post titles and tag based off origin subreddit.</li>
                </ol>
            </li>
            <li>Produce frequency/tfidf matrices and then project tfidf matrix to semantic space.
                <ol type="i">
                    <li>Use bundled post titles as raw data set.</li>
                    <li>Use <code class="code_inline">sklearn</code> to produce matrices.</li>
                </ol>
            </li>
            <li>Cluster post titles with k-means clustering.
                <ol type="i">
                    <li>Use <code class="code_inline">sklearn</code> to cluster post titles in semantic space.</li>
                    <li>Produce purity measurements of each cluster.</li>
                </ol>
            </li>
        </ol>
    </div>

    <div class="section_break"></div>

    <div class="header_block">
        <h2>Gathering and Processing Submission Titles From Reddit</h2>
    </div>

    

    <p class="indent">
        To gather submissions from Reddit, I used the Python package <a href="https://praw.readthedocs.io/en/latest/">PRAW</a> (Python Reddit API Wrapper). PRAW is an excellent package
        that allows for simple access to Reddit's API. It's easy to use and it allows you to gather up to 1000 submissions (per subreddit) at once. 
    </p>

    <p class="indent">
        Once you have signed up to access Reddit's API and have your credientials for OAuth2, we can have access to subreddit submissions with only a few lines of code. Here's the
        general idea:
    </p>

<pre><code class="Python code_output">#Use your credentials to log into your bot's account
reddit_bot.login()
                
#Create a subreddit object for a specific subreddit that contains all provided attributes of that subreddit
subreddit = reddit_bot.subreddit(subreddit_name)
                
#Print top level comments from x number of submissions inside of the subreddit
for submission in subreddit.top(limit = num_submissions):
    print(submission)
            
</code></pre>

    <p class="indent">
        For this project, I decided to keep it simple gather 1000 submissions from three different subreddits: Westworld, Doctor Who, and Game of Thrones. These are T.V. show fan base subreddits where character 
        names will be frequently mentioned. My hope is that LSA will pick up on this fact and be able to differentiate betwen the subreddits.
    </p>

    <p class="indent">
        While gathering submissions from each subreddit, I filter out all of the special characters along with stop words and remove capitalization to make it easier for the data to be processed when it comes to 
        applying LSA. By using <code class="code_inline">stop_words = set(list(text.ENGLISH_STOP_WORDS) +  list(stopwords.words("english")))</code> I was able to combine both the <code class="code_inline">nltk</code> and 
        <code class="code_inline">sklearn</code> stop word sets since individually, they did not seem to remove the breadth of stop words that I would have liked. Here's some sample output showing before/after 
        the filtering of the submisisons:
    </p>

<pre><code class="nohighlight code_output">
</code></pre>

    <p class="indent">
        After trying to gather 1000 submissions from each subreddit, I was able to successfully gather 2947 of the 3000 submissions (98.63%). This is due to the fact that some of the submissions were
        either empty or deleted. After gathering the submissions, I then put all 2947 submissions into a pandas data frame (what I refer to in my code as bundling) and then 
    </p>

<pre><code class="nohighlight code_output">
</code></pre>


    <p class="indent">
        When I refer to tagging and bundling, I am refering to the fact that I bundled all 2947 submissions into one pandas data frame and then created a separate column to tag each of the submissions with the proper subreddit
        in which the submission came from. This will be the raw data that is used when we run LSA.
    </p>


    <p class="indent">
        Now that I have successfully filtered, bundled and tagged the submissions, its time to do some initial analyis on this data by converting the raw data into their frequency and tfidf matrix representations. To
        do so, I used <code class="code_inline">sklearn</code>. The general idea to convert a raw text collection into its freqency or tfidf repesenation is rather straight-forward.
    </p>

<pre><code class="Python code_output">def get_frequency_matrix(raw_text):

    #Convert our raw text into a frequency matrix
    count_vectorizer = CountVectorizer(max_df=0.25, max_features=500)
    frequency_matrix = count_vectorizer.fit_transform(raw_text)

    #Get the feature names from our 
    feature_names = count_vectorizer.get_feature_names()
    frequency_df = pd.DataFrame(frequency_matrix.toarray(), columns=feature_names)

    return frequency_df, feature_names

def get_tfidf_matrix(feature_names, frequency_df):

    tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True)
    tfidf_matrix = tfidf_transformer.fit_transform(frequency_df)
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

    return tfidf_df
            
</code></pre>


<pre><code class="nohighlight code_output">
            
</code></pre>

    <p class="indent">
        From the frequency matrix, we can see what the most commonly occuring words for each subreddit are.
    </p>

     

    


    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>
<pre><code class="nohighlight code_output">
</code></pre>

    <p class="indent">
        Now that I have successfully filtered the submission titles, its time to create the frequency, tfidf, and reduced matrix.
    </p>

    

    <div class="graph">
        <iframe width="100%" height="800" margin-left: "auto" margin-right: "auto" frameborder="0" allowfullscreen src="//plot.ly/~BrysonSeiler/18.embed"></iframe>
    </div>

<pre><code class="nohighlight code_output">
</code></pre>

<p>
    Hm. Okay, well it did alright, but not perfect. It seems like a large ammount of the data clusters into that corner. 
</p>

</div>
    </div>

    <div class="footer">
        <div class="footer-top">
            <a href="Bryson.Seiler@gmail.com">Email</a>
            <a href="https://github.com/BrysonSeiler">GitHub</a>
            <a href="https://www.linkedin.com/in/brysonseiler/">LinkedIn</a> 
        </div>
        <div class="footer-bottom">
            <p>
                This website was influenced by
                <a href="http://jmcglone.com/guides/github-pages/"> Jonathan McGlone's guide </a> to hosting a personal site on GitHub. <br/>
                Code highlighting by <a href="https://highlightjs.org/"> Highlight.js</a> (<a href="https://github.com/highlightjs/highlight.js/blob/master/LICENSE">BSD license</a>).<br/>
                <a href="https://fonts.google.com/"> Google Fonts</a>: Roboto, Fira Mono.
            </p>
        </div>
    </div>

</body>

</html>